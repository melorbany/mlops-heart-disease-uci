name: CI/CD

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

env:
  PYTHON_VERSION: "3.13"
  DOCKER_IMAGE: "${{ vars.DOCKERHUB_USERNAME }}/heart-predict-api"
  APP_CONTAINER_NAME: "heart-predict-api"

jobs:
  # ==========================================
  # CODE LINTING 
  # ==========================================
  lint:
    name: Code Linting
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
            pip install flake8 pylint black isort autopep8
    
      - name: Use Flake8 (PEP 8 Style Enforcement)
        run: |
          {
            echo "### 1. Flake8 (PEP 8 Style Enforcement)"
            echo '```'
            flake8 src/ \
              --max-line-length=100 \
              --max-complexity=10 \
              --statistics \
              --count \
              --show-source \
              --exclude=__pycache__,*.pyc,.git,__init__.py,migrations || true
            echo '```'
            echo ""
          }

      - name: Validate Code Quality
        run: |
          {
            echo "### 2. Pylint (Code Quality)"
            echo '```'
            pylint src/ \
              --output-format=text \
              --reports=y \
              --disable=C0111,C0103,R0903,W0212 \
              --fail-under=7.0 \
              --max-line-length=100 || true
            echo '```'
            echo ""
          }

      - name: Check Code Formatting
        run: |
          {
            echo "### 3. Black (Code Formatting)"
            echo '```'
            black --check --diff --line-length=100 src/ || true
            echo '```'
            echo ""
            echo "Hint: run 'black --line-length=100 src/' locally to auto-format if needed."
            echo ""
          }

      - name: Check Import Order
        run: |
          {
            echo "### 4. isort (Import Organization)"
            echo '```'
            # Check-only, do not modify code
            isort --check-only --diff --profile black src/ || true
            echo '```'
            echo ""
            echo "Hint: run 'isort --profile black src/' locally to auto-sort imports if needed."
            echo ""
          }
    

  # ==========================================
  # SECURITY ANALYSIS
  # ========================================== 
  security:
    name: Security Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install bandit safety pip-audit

      - name: Security Linter
        run: |
          {
            echo "### 1. Bandit (Python Security Linter)"
            echo '```'
            bandit -r src/ \
              -f txt \
              --severity-level medium \
              --confidence-level medium \
              --skip B101,B601 \
              -ll || true
            echo '```'
            echo ""
          }

      - name: Check Dependency Vulnerabilities
        run: |
          pip freeze > requirements-frozen.txt

          {
            echo "### 2. Safety (Dependency Vulnerabilities)"
            echo '```'
            safety check \
              --file requirements-frozen.txt \
              --output text || true
            echo '```'
            echo ""
          }

      - name: Check Package Security
        run: |
          {
            echo "### 3. pip-audit (PyPI Package Security)"
            echo '```'
            pip-audit \
              --requirement requirements-frozen.txt \
              --format json \
              --output pip-audit-report.json || true
            echo '```'
            echo ""
          }

          mkdir -p security-reports
          mv pip-audit-report.json security-reports/pip-audit-report.json || true

      - name: Secret Detection
        run: |
          SECRET_PATTERNS=(
            "password\s*=\s*['\"].*['\"]"
            "api[_-]?key\s*=\s*['\"].*['\"]"
            "secret\s*=\s*['\"].*['\"]"
            "token\s*=\s*['\"].*['\"]"
            "aws[_-]?(access|secret)[_-]?key"
          )

          : > secrets-report.txt

          for pattern in "${SECRET_PATTERNS[@]}"; do
            grep -r -i -E "$pattern" src/ --exclude-dir=__pycache__ >> secrets-report.txt 2>&1 || true
          done

          {
            echo "### 5. Secret Detection"
            echo '```'
            if [ -s secrets-report.txt ]; then
              cat secrets-report.txt
              echo ""
              echo "Potential hardcoded secrets found."
              echo "Please review and use environment variables or secret managers instead."
            else
              echo "No hardcoded secrets detected."
            fi
            echo '```'
            echo ""
          }

  # ==========================================
  # DATA FETCH & PROCESS
  # ==========================================
  data:
    name: Data Fetch & Process
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Clean previous data
        run: |
          echo "Cleaning previous data directories..."
          rm -rf data/raw data/processed artifacts models || true
          mkdir -p data/raw data/processed artifacts models

      - name: Download dataset
        run: |
          echo "Running: python -m src.data.download_data"
          python -m src.data.download_data

      - name: Convert UCI data to CSV
        run: |
          echo "Running: python -m src.data.convert_uci_to_csv"
          python -m src.data.convert_uci_to_csv

      - name: Preprocess CSV files
        run: |
          echo "Running: python -m src.data.preprocess"
          python -m src.data.preprocess

      - name: Verify processed dataset
        run: |
          if [ ! -f "data/processed/heart_clean.csv" ]; then
            echo "ERROR: data/processed/heart_clean.csv not found after preprocessing."
            exit 1
          fi

      - name: Upload processed dataset artifact
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/heart_clean.csv
          if-no-files-found: error

  # ==========================================
  # EDA VISUALIZATIONS
  # ==========================================
  eda:
    name: EDA Visualizations
    runs-on: ubuntu-latest
    needs: data

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download processed dataset artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed

      - name: Run EDA and generate visualizations
        run: |
          set -euo pipefail
          echo "Running: python -m src.eda.visualize"
          python -m src.eda.visualize

      - name: Verify EDA outputs
        run: |
          set -euo pipefail
          echo "Checking EDA artifacts..."
          ls -lah artifacts/eda || true

          test -f "artifacts/eda/histograms.png" || (echo "ERROR: histograms.png not found" && exit 1)
          test -f "artifacts/eda/correlation_heatmap.png" || (echo "ERROR: correlation_heatmap.png not found" && exit 1)
          test -f "artifacts/eda/class_balance.png" || (echo "ERROR: class_balance.png not found" && exit 1)

          echo "All EDA plots generated successfully"

      - name: Display EDA visualizations in workflow summary
        shell: bash
        run: |
          set -euo pipefail

          echo "## Exploratory Data Analysis (EDA)" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "Automated EDA visualizations generated from the processed dataset." >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          # Portable base64 (ubuntu supports -w; mac uses -b)
          b64() {
            if base64 --help 2>/dev/null | grep -q -- "-w"; then
              base64 -w 0 "$1"
            else
              base64 -b 0 "$1"
            fi
          }

          add_image_to_summary() {
            local title="$1"
            local image_path="$2"

            echo "### $title" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"

            if [[ -f "$image_path" ]]; then
              local extension="${image_path##*.}"
              local mime_type="image/png"
              case "$extension" in
                jpg|jpeg) mime_type="image/jpeg" ;;
                png) mime_type="image/png" ;;
              esac

              # Convert image to base64 and embed as data URL
              local base64_image
              base64_image="$(b64 "$image_path")"

              echo "<img src=\"data:${mime_type};base64,${base64_image}\" alt=\"$title\" style=\"max-width: 100%; height: auto;\" />" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "**Status:** Embedded successfully" >> "$GITHUB_STEP_SUMMARY"
            else
              echo "**Status:** Image not found at \`$image_path\`" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "**Debug (workspace):**" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
              pwd >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              ls -lah >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              ls -lah artifacts 2>&1 >> "$GITHUB_STEP_SUMMARY" || true
              echo "" >> "$GITHUB_STEP_SUMMARY"
              ls -lah artifacts/eda 2>&1 >> "$GITHUB_STEP_SUMMARY" || true
              echo '```' >> "$GITHUB_STEP_SUMMARY"
            fi

            echo "" >> "$GITHUB_STEP_SUMMARY"
          }

          add_image_to_summary "1. Class Balance Distribution" "artifacts/eda/class_balance.png"
          add_image_to_summary "2. Feature Correlation Heatmap" "artifacts/eda/correlation_heatmap.png"
          add_image_to_summary "3. Numeric Features Histograms" "artifacts/eda/histograms.png"

          if [[ -f "artifacts/eda/eda_summary.txt" ]]; then
            echo "### EDA Summary" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
            cat artifacts/eda/eda_summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "---" >> "$GITHUB_STEP_SUMMARY"
          echo "_EDA visualizations help understand data distribution, correlations, and potential issues before model training._" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload EDA artifacts
        uses: actions/upload-artifact@v4
        with:
          name: eda-visualizations
          path: artifacts/eda/
          if-no-files-found: error

  # ==========================================
  # TRAIN MODEL
  # ==========================================
  train-model:
    name: Train Model
    runs-on: ubuntu-latest
    needs: data

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download processed dataset artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed

      - name: Train model
        env:
          PROCESSED_DATA_PATH: data/processed/heart_clean.csv
        run: |
          echo "Using processed data at: $PROCESSED_DATA_PATH"
          echo "Running: python -m src.models.train_model"
          python -m src.models.train_model

      - name: Upload trained model artifact
        uses: actions/upload-artifact@v4
        with:
          name: heart-model
          path: models/
          if-no-files-found: error
      - name: Upload MLflow tracking DB
        uses: actions/upload-artifact@v4
        with:
          name: mlflow-db
          path: mlflow/mlflow.db         
          if-no-files-found: error

  # ==========================================
  # MLFLOW REPORT
  # ==========================================
  mlflow-report:
    name: MLflow Report
    runs-on: ubuntu-latest
    needs: train-model

    steps:
      - name: Download MLflow tracking DB
        uses: actions/download-artifact@v4
        with:
          name: mlflow-db
          path: mlflow-db

      - name: Show MLflow runs in job summary (rich)
        shell: bash
        run: |
          set -euo pipefail

          echo "## MLflow Experiment Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          DB_PATH="mlflow-db/mlflow.db"

          if [ ! -f "$DB_PATH" ]; then
            echo "**MLflow DB not found** at \`$DB_PATH\`" >> "$GITHUB_STEP_SUMMARY"
            exit 1
          fi

          echo "**Using database:** \`$DB_PATH\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          has_table () {
            local t="$1"
            sqlite3 "$DB_PATH" "SELECT name FROM sqlite_master WHERE type='table' AND name='$t';" | grep -q "^$t$"
          }

          run_sql () {
            local title="$1"
            local sql="$2"

            echo "### $title" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"

            local csv
            csv="$(sqlite3 -header -csv "$DB_PATH" "$sql" || true)"
            if [ -z "$csv" ]; then
              echo "_No data found (or schema mismatch for your MLflow version)._ " >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              return 0
            fi

            local header
            header="$(echo "$csv" | head -n 1)"
            IFS=',' read -r -a cols <<< "$header"

            # Header row
            printf -- '|' >> "$GITHUB_STEP_SUMMARY"
            for c in "${cols[@]}"; do
              printf -- ' %s |' "$c" >> "$GITHUB_STEP_SUMMARY"
            done
            printf -- '\n' >> "$GITHUB_STEP_SUMMARY"

            printf -- '|' >> "$GITHUB_STEP_SUMMARY"
            for _ in "${cols[@]}"; do
              printf -- '---|' >> "$GITHUB_STEP_SUMMARY"
            done
            printf -- '\n' >> "$GITHUB_STEP_SUMMARY"

            # Data rows
            echo "$csv" | tail -n +2 | while IFS=',' read -r -a row; do
              printf -- '|' >> "$GITHUB_STEP_SUMMARY"
              for cell in "${row[@]}"; do
                cell="${cell//|/\\|}"
                printf -- ' %s |' "$cell" >> "$GITHUB_STEP_SUMMARY"
              done
              printf -- '\n' >> "$GITHUB_STEP_SUMMARY"
            done

            echo "" >> "$GITHUB_STEP_SUMMARY"
          }

          echo "### DB / Schema" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "**SQLite tables detected:**" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo '```text' >> "$GITHUB_STEP_SUMMARY"
          sqlite3 "$DB_PATH" ".tables" >> "$GITHUB_STEP_SUMMARY" || true
          echo '```' >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          if has_table experiments; then
            run_sql "Experiments" "
              SELECT experiment_id, name, artifact_location, lifecycle_stage
              FROM experiments
              ORDER BY CAST(experiment_id AS INTEGER) ASC;
            "
          else
            echo "_Table \`experiments\` not found._" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          if has_table runs; then
            run_sql "Latest Runs (detailed)" "
              SELECT
                run_uuid AS run_id,
                experiment_id,
                status,
                lifecycle_stage,
                user_id,
                start_time,
                end_time,
                artifact_uri
              FROM runs
              ORDER BY start_time DESC
              LIMIT 10;
            "
          else
            echo "_Table \`runs\` not found._" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
            exit 1
          fi

          if has_table params; then
            run_sql "Params (latest runs)" "
              SELECT p.run_uuid AS run_id, p.key, p.value
              FROM params p
              WHERE p.run_uuid IN (SELECT run_uuid FROM runs ORDER BY start_time DESC LIMIT 10)
              ORDER BY p.run_uuid, p.key;
            "
          else
            echo "_Table \`params\` not found._" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          if has_table metrics; then
            run_sql "Metrics (latest value per key for latest runs)" "
              WITH latest AS (
                SELECT m.run_uuid, m.key, MAX(COALESCE(m.step, 0)) AS max_step
                FROM metrics m
                WHERE m.run_uuid IN (SELECT run_uuid FROM runs ORDER BY start_time DESC LIMIT 10)
                GROUP BY m.run_uuid, m.key
              ),
              latest2 AS (
                SELECT m.run_uuid, m.key, m.value, m.step, m.timestamp
                FROM metrics m
                JOIN latest l
                  ON l.run_uuid = m.run_uuid
                AND l.key = m.key
                AND COALESCE(l.max_step, 0) = COALESCE(m.step, 0)
                WHERE m.run_uuid IN (SELECT run_uuid FROM runs ORDER BY start_time DESC LIMIT 10)
              )
              SELECT run_uuid AS run_id, key, value, step, timestamp
              FROM latest2
              ORDER BY run_uuid, key;
            "
          else
            echo "_Table \`metrics\` not found._" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          if has_table tags; then
            run_sql "Tags (latest runs)" "
              SELECT t.run_uuid AS run_id, t.key, t.value
              FROM tags t
              WHERE t.run_uuid IN (SELECT run_uuid FROM runs ORDER BY start_time DESC LIMIT 10)
              ORDER BY t.run_uuid, t.key;
            "
          elif has_table run_tags; then
            run_sql "Tags (latest runs)" "
              SELECT t.run_uuid AS run_id, t.key, t.value
              FROM run_tags t
              WHERE t.run_uuid IN (SELECT run_uuid FROM runs ORDER BY start_time DESC LIMIT 10)
              ORDER BY t.run_uuid, t.key;
            "
          else
            echo "_No tags table found (\`tags\` or \`run_tags\`)._" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          run_sql "Artifact URIs (latest runs)" "
            SELECT run_uuid AS run_id, artifact_uri
            FROM runs
            ORDER BY start_time DESC
            LIMIT 10;
          "

          if has_table registered_models; then
            run_sql "Registered Models" "
              SELECT name, creation_time, last_updated_time, description
              FROM registered_models
              ORDER BY last_updated_time DESC
              LIMIT 20;
            "
          fi

          if has_table model_versions; then
            run_sql "Model Versions" "
              SELECT name, version, creation_time, last_updated_time, current_stage, status, run_id, source
              FROM model_versions
              ORDER BY last_updated_time DESC
              LIMIT 50;
            "
          fi

          echo "---" >> "$GITHUB_STEP_SUMMARY"
          echo "_Note: MLflow artifacts (files) are not stored in the SQLite DB; only the artifact URI is._" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"  

# ==========================================
  # TESTING & COVERAGE
  # ==========================================
  test:
    name: Tests & Coverage
    runs-on: ubuntu-latest
    needs: train-model

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download trained model artifact
        uses: actions/download-artifact@v4
        with:
          name: heart-model
          path: models/

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-html pytest-xdist pytest-timeout coverage

      - name: Initialize test summary file
        run: |
          echo "Test and Coverage Results" > test-summary.txt
          echo "" >> test-summary.txt
          echo "## Test and Coverage Results" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

      - name: Run Unit Tests with Coverage
        id: unit-tests
        shell: bash
        run: |
          set -o pipefail

          echo "### 1. Test Execution" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

          pytest tests/ \
            -v \
            --cov=src \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html:htmlcov \
            --cov-report=json:coverage.json \
            --html=test-report.html \
            --self-contained-html \
            --junitxml=junit.xml \
            --maxfail=5 \
            --tb=short \
            -n auto 2>&1 | tee test-output.txt

          tail -30 test-output.txt | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

          echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

      - name: Generate Coverage Report
        if: always()
        shell: bash
        run: |
          echo "### 2. Code Coverage Report" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

          if [ -f coverage.json ]; then
            COVERAGE=$(python3 -c "import json; data=json.load(open('coverage.json')); print(f\"{data['totals']['percent_covered']:.2f}\")" 2>/dev/null || echo "0")

            echo "**Total Coverage:** ${COVERAGE}%" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

            echo "#### 2.1 Detailed Coverage by Module" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            coverage report --skip-covered >> coverage-report.txt 2>/dev/null || echo "Coverage report not available" >> coverage-report.txt
            cat coverage-report.txt | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

            echo "#### 2.2 Files with Missing Coverage" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            coverage report --skip-covered --show-missing > coverage-missing.txt 2>/dev/null || echo "Coverage report not available" > coverage-missing.txt

            if grep -v "100%" coverage-missing.txt > coverage-missing-filtered.txt 2>/dev/null && [ -s coverage-missing-filtered.txt ]; then
              cat coverage-missing-filtered.txt | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            else
              echo "All files have 100% coverage." | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            fi

            echo '```' | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
            echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          else
            echo "Coverage data not available" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Check Coverage Threshold
        shell: bash
        run: |
          echo "### 3. Coverage Threshold Check" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

          if [ -f coverage.json ]; then
            COVERAGE=$(python3 -c "import json; print(json.load(open('coverage.json'))['totals']['percent_covered'])" 2>/dev/null || echo 0)
            THRESHOLD=60

            if (( $(echo "$COVERAGE < $THRESHOLD" | bc -l) )); then
              MSG="Coverage (${COVERAGE}%) is below threshold (${THRESHOLD}%)"
              echo "$MSG" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
              echo "$MSG"
              exit 1
            else
              MSG="Coverage (${COVERAGE}%) meets threshold (${THRESHOLD}%)"
              echo "$MSG" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
              echo "$MSG"
            fi
          else
            echo "Coverage threshold check skipped: coverage.json not found" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

      - name: Final Test Summary
        if: success()
        run: |
          echo "---" | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"
          echo "All tests passed and coverage meets the required threshold." | tee -a test-summary.txt >> "$GITHUB_STEP_SUMMARY"

      - name: Upload test and coverage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: |
            test-summary.txt
            test-output.txt
            coverage.xml
            junit.xml
            htmlcov/
            test-report.html
          compression-level: 6
          
  # ==========================================
  # VALIDATE MODEL
  # ==========================================
  validate-model:
    name: Validate Model
    runs-on: ubuntu-latest
    needs:
      - train-model

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download trained model artifact
        uses: actions/download-artifact@v4
        with:
          name: heart-model
          path: models/


  # ==========================================
  # DOCKER BUILD & VALIDATE
  # ==========================================
  docker-build:
    name: Docker Build & Validate
    runs-on: ubuntu-latest
    needs:
      - lint
      - security
      - test
      - validate-model
      - eda

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download trained model artifact
        uses: actions/download-artifact@v4
        with:
          name: heart-model
          path: models/

      - name: Set Docker image name
        id: image-name
        run: |
          echo "IMAGE=${DOCKER_IMAGE}" >> "$GITHUB_OUTPUT"

      - name: Build Docker image (candidate)
        run: |
          docker build \
            -f docker/Dockerfile \
            -t ${{ steps.image-name.outputs.IMAGE }}:candidate \
            .

      - name: Run container for validation
        run: |
          docker stop ${{ env.APP_CONTAINER_NAME }} || true
          docker rm ${{ env.APP_CONTAINER_NAME }} || true

          docker run -d \
            --name ${{ env.APP_CONTAINER_NAME }} \
            -p 8000:8000 \
            ${{ steps.image-name.outputs.IMAGE }}:candidate

          echo "Container started, waiting for app to be ready..."
          sleep 20

      - name: Validate Docker image endpoint
        run: |
          echo "Calling http://127.0.0.1:8000/health"
          curl --fail --max-time 30 http://127.0.0.1:8000/health | tee health_response.json

          if command -v jq >/dev/null 2>&1; then
            status=$(jq -r '.status' health_response.json)
            if [ "$status" != "ok" ]; then
              echo "Expected status=ok but got: $status"
              exit 1
            fi
          else
            if ! grep -q '"status"' health_response.json || ! grep -q '"ok"' health_response.json; then
              echo "Response does not contain expected status=ok"
              cat health_response.json
              exit 1
            fi
          fi

          echo "Local container health check passed."

      - name: Stop and remove validation container
        if: always()
        run: |
          docker stop ${{ env.APP_CONTAINER_NAME }} || true
          docker rm ${{ env.APP_CONTAINER_NAME }} || true  

  # ==========================================
  # PUSH TO DOCKER HUB
  # ==========================================
  docker-push:
    name: Push to Docker Hub
    runs-on: ubuntu-latest
    needs: docker-build

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download trained model artifact
        uses: actions/download-artifact@v4
        with:
          name: heart-model
          path: models/

      - name: Set Docker image name
        id: image-name
        run: |
          echo "IMAGE=${DOCKER_IMAGE}" >> "$GITHUB_OUTPUT"

      - name: Compute short SHA
        id: short-sha
        run: |
          SHORT_SHA="${GITHUB_SHA::8}"
          echo "SHORT_SHA=${SHORT_SHA}" >> "$GITHUB_OUTPUT"

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/Dockerfile
          push: true
          tags: |
            ${{ steps.image-name.outputs.IMAGE }}:latest
            ${{ steps.image-name.outputs.IMAGE }}:${{ steps.short-sha.outputs.SHORT_SHA }}

      - name: Show pushed image tags
        run: |
          echo "Pushed Docker image:"
          echo "  ${{ steps.image-name.outputs.IMAGE }}:latest"
          echo "  ${{ steps.image-name.outputs.IMAGE }}:${{ steps.short-sha.outputs.SHORT_SHA }}"
  
  # ==========================================
  # DEPLOY TO CLOUD
  # ==========================================
  cloud-deploy:
    name: Deploy to Cloud
    runs-on: ubuntu-latest
    needs: docker-push
    if: github.ref == 'refs/heads/master'

    steps:
      - name: Set Docker image name
        id: image-name
        run: |
          echo "IMAGE=${DOCKER_IMAGE}" >> "$GITHUB_OUTPUT"

      - name: Install SSH client
        run: |
          sudo apt-get update
          sudo apt-get install -y openssh-client curl

      - name: Configure SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.CLOUD_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          printf "Host cloud-server\n  HostName %s\n  User %s\n  IdentityFile ~/.ssh/id_rsa\n  StrictHostKeyChecking no\n" \
            "${{ secrets.CLOUD_HOST }}" "${{ secrets.CLOUD_SSH_USER }}" >> ~/.ssh/config

      - name: Ensure Docker is installed on cloud server
        run: |
          ssh cloud-server << 'EOF'
          set -e

          echo "=== Checking if Docker is installed ==="
          if ! command -v docker >/dev/null 2>&1; then
            echo "Docker not found. Installing Docker..."
            curl -fsSL https://get.docker.com | sh
          else
            echo "Docker is already installed."
          fi
          EOF

      - name: Stop and remove existing app container on cloud server
        run: |
          ssh cloud-server << 'EOF'
          set -e

          echo "=== Cleaning up old containers for this app ==="
          if docker ps -a --format '{{.Names}}' | grep -wq "${{ env.APP_CONTAINER_NAME }}"; then
            echo "Stopping and removing existing container ${{ env.APP_CONTAINER_NAME }}..."
            docker stop ${{ env.APP_CONTAINER_NAME }} || true
            docker rm ${{ env.APP_CONTAINER_NAME }} || true
          else
            echo "No existing container named ${{ env.APP_CONTAINER_NAME }}."
          fi
          EOF

      - name: Remove old app images on cloud server
        run: |
          ssh cloud-server << 'EOF'
          set -e

          echo "=== Cleaning old images for this app (if any) ==="
          PROJECT_IMAGE="${{ steps.image-name.outputs.IMAGE }}"
          old_images=$(docker images --format '{{.Repository}}:{{.Tag}}' | grep "^$PROJECT_IMAGE" || true)
          if [ -n "$old_images" ]; then
            echo "Found old images:"
            echo "$old_images"
            echo "Removing old images..."
            echo "$old_images" | xargs -r docker rmi || true
          else
            echo "No old images for $PROJECT_IMAGE."
          fi
          EOF

      - name: Pull latest image on cloud server
        run: |
          ssh cloud-server << 'EOF'
          set -e

          echo "=== Pulling latest image from registry ==="
          docker pull ${{ steps.image-name.outputs.IMAGE }}:latest
          EOF

      - name: Run new app container on cloud server
        run: |
          ssh cloud-server << 'EOF'
          set -e

          echo "=== Running new container ==="
          docker run -d \
            --name ${{ env.APP_CONTAINER_NAME }} \
            -p 80:8000 \
            --restart always \
            ${{ steps.image-name.outputs.IMAGE }}:latest

          echo "Deployment on cloud server completed."
          EOF

  # ==========================================
  # HEALTH CHECK
  # ==========================================
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    needs: cloud-deploy
    if: github.ref == 'refs/heads/master'

    steps:
      - name: Install curl
        run: |
          sudo apt-get update
          sudo apt-get install -y curl

      - name: Check API /health on server
        run: |
          HEALTH_URL="http://${{ secrets.CLOUD_HOST }}/health"
          echo "Checking health at: $HEALTH_URL"

          for i in {1..10}; do
            if curl --silent --fail "$HEALTH_URL" -o server_health_response.json; then
              echo "Health endpoint is responding."
              cat server_health_response.json
              break
            else
              echo "Health check failed, retrying in 5s... (attempt $i/10)"
              sleep 5
            fi
          done

          if [ ! -f server_health_response.json ]; then
            echo "Health endpoint did not respond after retries."
            exit 1
          fi

          if ! grep -q '"status"' server_health_response.json || ! grep -q '"ok"' server_health_response.json; then
            echo "Server health response does not contain expected status=ok"
            cat server_health_response.json
            exit 1
          fi

          echo "Cloud server health check passed."


  # ==========================================
  # TEST PREDICT API
  # ==========================================
  predict-api:
    name: Test Predict API
    runs-on: ubuntu-latest
    needs: cloud-deploy
    if: github.ref == 'refs/heads/master'

    steps:
      - name: Install curl and jq
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq

      - name: Check API /health on server
        run: |
          HEALTH_URL="http://${{ secrets.CLOUD_HOST }}/health"
          echo "Checking health at: $HEALTH_URL"

          for i in {1..10}; do
            if curl --silent --fail "$HEALTH_URL" -o server_health_response.json; then
              echo "Health endpoint is responding."
              cat server_health_response.json
              break
            else
              echo "Health check failed, retrying in 5s... (attempt $i/10)"
              sleep 5
            fi
          done

          if [ ! -f server_health_response.json ]; then
            echo "Health endpoint did not respond after retries."
            exit 1
          fi

          if ! grep -q '"status"' server_health_response.json || ! grep -q '"ok"' server_health_response.json; then
            echo "Server health response does not contain expected status=ok"
            cat server_health_response.json
            exit 1
          fi

          echo "Cloud server health check passed."

      - name: Test /predict endpoint
        run: |
          PREDICT_URL="http://${{ secrets.CLOUD_HOST }}/predict"
          echo "Testing predict endpoint at: $PREDICT_URL"
          
          # Sample patient data for prediction
          SAMPLE_DATA='{
            "age": 63,
            "sex": 1,
            "cp": 3,
            "trestbps": 145,
            "chol": 233,
            "fbs": 1,
            "restecg": 0,
            "thalach": 150,
            "exang": 0,
            "oldpeak": 2.3,
            "slope": 0,
            "ca": 0,
            "thal": 1
          }'
          
          echo "Sending prediction request with sample data:"
          echo "$SAMPLE_DATA" | jq '.'
          
          # Make prediction request
          HTTP_STATUS=$(curl -s -o predict_response.json -w "%{http_code}" \
            -X POST \
            -H "Content-Type: application/json" \
            -d "$SAMPLE_DATA" \
            "$PREDICT_URL")
          
          echo "HTTP Status Code: $HTTP_STATUS"
          
          if [ "$HTTP_STATUS" -ne 200 ]; then
            echo "Prediction request failed with status $HTTP_STATUS"
            cat predict_response.json
            exit 1
          fi
          
          echo "Prediction response:"
          cat predict_response.json | jq '.'
          
          # Validate response structure
          if ! jq -e '.prediction' predict_response.json > /dev/null; then
            echo "Response missing 'prediction' field"
            exit 1
          fi
          
          if ! jq -e '.probability' predict_response.json > /dev/null; then
            echo "Response missing 'probability' field"
            exit 1
          fi
          
          # Extract and validate values
          PREDICTION=$(jq -r '.prediction' predict_response.json)
          PROBABILITY=$(jq -r '.probability' predict_response.json)
          
          echo "Extracted values:"
          echo "  Prediction: $PREDICTION"
          echo "  Probability: $PROBABILITY"
          
          # Validate prediction is 0 or 1
          if [ "$PREDICTION" != "0" ] && [ "$PREDICTION" != "1" ]; then
            echo "Invalid prediction value: $PREDICTION (expected 0 or 1)"
            exit 1
          fi
          
          # Validate probability is between 0 and 1
          if ! echo "$PROBABILITY" | awk '{exit !($1 >= 0 && $1 <= 1)}'; then
            echo "Invalid probability value: $PROBABILITY (expected 0.0 to 1.0)"
            exit 1
          fi
          
          echo "[PASS] Prediction API test passed successfully!"
          echo "   Prediction: $PREDICTION ($([ "$PREDICTION" = "1" ] && echo "Disease" || echo "No Disease"))"
          echo "   Probability: $PROBABILITY ($(echo "$PROBABILITY * 100" | bc)%)"

      - name: Test /predict endpoint with multiple samples
        run: |
          PREDICT_URL="http://${{ secrets.CLOUD_HOST }}/predict"
          echo "Testing predict endpoint with multiple sample cases..."
          
          # Test Case 1: Low-risk patient
          echo ""
          echo "--- Test Case 1: Low-risk patient ---"
          LOW_RISK_DATA='{
            "age": 40,
            "sex": 0,
            "cp": 0,
            "trestbps": 120,
            "chol": 200,
            "fbs": 0,
            "restecg": 0,
            "thalach": 170,
            "exang": 0,
            "oldpeak": 0.0,
            "slope": 0,
            "ca": 0,
            "thal": 2
          }'
          
          curl -s -X POST \
            -H "Content-Type: application/json" \
            -d "$LOW_RISK_DATA" \
            "$PREDICT_URL" | jq '.'
          
          # Test Case 2: High-risk patient
          echo ""
          echo "--- Test Case 2: High-risk patient ---"
          HIGH_RISK_DATA='{
            "age": 67,
            "sex": 1,
            "cp": 0,
            "trestbps": 160,
            "chol": 286,
            "fbs": 0,
            "restecg": 0,
            "thalach": 108,
            "exang": 1,
            "oldpeak": 1.5,
            "slope": 1,
            "ca": 3,
            "thal": 2
          }'
          
          curl -s -X POST \
            -H "Content-Type: application/json" \
            -d "$HIGH_RISK_DATA" \
            "$PREDICT_URL" | jq '.'
          
          echo ""
          echo "[PASS] Multiple sample tests completed successfully!"

      - name: Test error handling
        run: |
          PREDICT_URL="http://${{ secrets.CLOUD_HOST }}/predict"
          echo "Testing error handling with invalid data..."
          
          # Test with missing field
          INVALID_DATA='{
            "age": 63,
            "sex": 1
          }'
          
          HTTP_STATUS=$(curl -s -o error_response.json -w "%{http_code}" \
            -X POST \
            -H "Content-Type: application/json" \
            -d "$INVALID_DATA" \
            "$PREDICT_URL")
          
          echo "HTTP Status Code for invalid data: $HTTP_STATUS"
          
          if [ "$HTTP_STATUS" -eq 200 ]; then
            echo "[WARNING] API accepted invalid data (should return 400 or 422)"
          else
            echo "[PASS] API correctly rejected invalid data"
            cat error_response.json | jq '.' || cat error_response.json
          fi
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Heart Disease UCI Dataset\n",
    "\n",
    "**Author:** MLOps Team  \n",
    "**Date:** January 2026  \n",
    "**Dataset:** UCI Heart Disease (Combined: Cleveland, Hungarian, Switzerland, VA)\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis (EDA) on the Heart Disease dataset to:\n",
    "\n",
    "1. Understand the data distribution and quality\n",
    "2. Identify missing values and outliers\n",
    "3. Analyze feature correlations\n",
    "4. Examine class balance\n",
    "5. Generate insights for model development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    PROCESSED_DATA_PATH,\n",
    "    TARGET_COLUMN,\n",
    "    NUMERIC_FEATURES,\n",
    "    CATEGORICAL_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"Data path: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")\n",
    "print(f\"\\nNumeric features ({len(NUMERIC_FEATURES)}): {NUMERIC_FEATURES}\")\n",
    "print(f\"\\nCategorical features ({len(CATEGORICAL_FEATURES)}): {CATEGORICAL_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "df = pd.read_csv(PROCESSED_DATA_PATH, na_values='?')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for numeric features\n",
    "df[NUMERIC_FEATURES].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Missing Values Detected:\")\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    missing_df['Missing Count'].plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Number of Missing Values')\n",
    "    ax.set_title('Missing Values by Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df[TARGET_COLUMN].value_counts().sort_index()\n",
    "class_pct = (class_counts / len(df) * 100).round(2)\n",
    "\n",
    "print(f\"Target Variable: {TARGET_COLUMN}\\n\")\n",
    "print(\"Class Distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"  Class {cls}: {count} samples ({class_pct[cls]}%)\")\n",
    "\n",
    "print(f\"\\nBalance Ratio: {class_counts.min() / class_counts.max():.2f}\")\n",
    "if class_counts.min() / class_counts.max() > 0.8:\n",
    "    print(\"‚úÖ Dataset is well balanced\")\n",
    "elif class_counts.min() / class_counts.max() > 0.5:\n",
    "    print(\"‚ö†Ô∏è  Slight imbalance, but acceptable\")\n",
    "else:\n",
    "    print(\"‚ùå Significant class imbalance detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class balance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "class_counts.plot(kind='bar', ax=ax1, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax1.set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xticklabels(['No Disease (0)', 'Disease (1)'], rotation=0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, v in enumerate(class_counts):\n",
    "    ax1.text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(\n",
    "    class_counts,\n",
    "    labels=['No Disease (0)', 'Disease (1)'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    explode=(0.05, 0.05),\n",
    "    textprops={'fontsize': 11, 'fontweight': 'bold'}\n",
    ")\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Numeric Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for all numeric features\n",
    "numeric_cols = [col for col in NUMERIC_FEATURES if col in df.columns]\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_val = df[col].mean()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
    "    \n",
    "    ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(col, fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(numeric_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Numeric Features: Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary with skewness and kurtosis\n",
    "stats_df = df[numeric_cols].agg([\n",
    "    'mean', 'median', 'std', 'min', 'max',\n",
    "    lambda x: x.skew(),  # skewness\n",
    "    lambda x: x.kurtosis()  # kurtosis\n",
    "]).T\n",
    "\n",
    "stats_df.columns = ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Skewness', 'Kurtosis']\n",
    "stats_df = stats_df.round(2)\n",
    "\n",
    "print(\"\\nüìä Numeric Features: Statistical Summary\")\n",
    "display(stats_df)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  - Skewness: < -1 or > 1 indicates high skew, -0.5 to 0.5 is fairly symmetric\")\n",
    "print(\"  - Kurtosis: > 3 indicates heavy tails (outliers), < 3 indicates light tails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical features\n",
    "categorical_cols = [col for col in CATEGORICAL_FEATURES if col in df.columns]\n",
    "\n",
    "print(\"\\nüìã Categorical Features: Value Counts\\n\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    value_counts = df[col].value_counts().sort_index()\n",
    "    for val, count in value_counts.items():\n",
    "        pct = (count / len(df) * 100)\n",
    "        print(f\"  {val}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "n_cols = 4\n",
    "n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    ax = axes[idx]\n",
    "    value_counts = df[col].value_counts().sort_index()\n",
    "    \n",
    "    value_counts.plot(kind='bar', ax=ax, color='teal', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(value_counts):\n",
    "        ax.text(i, v + 2, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(categorical_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Categorical Features: Distribution', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix (numeric features + target)\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    corr_cols = numeric_cols + [TARGET_COLUMN]\n",
    "else:\n",
    "    corr_cols = numeric_cols\n",
    "\n",
    "correlation_matrix = df[corr_cols].corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with target variable\n",
    "if TARGET_COLUMN in correlation_matrix.columns:\n",
    "    target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ Top Features Correlated with {TARGET_COLUMN}:\\n\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Positive correlations\n",
    "    top_positive = target_corr.head(5)\n",
    "    top_positive.plot(kind='barh', ax=ax1, color='forestgreen', edgecolor='black')\n",
    "    ax1.set_title('Top 5 Positive Correlations', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Correlation Coefficient')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Negative correlations\n",
    "    top_negative = target_corr.tail(5)\n",
    "    top_negative.plot(kind='barh', ax=ax2, color='crimson', edgecolor='black')\n",
    "    ax2.set_title('Top 5 Negative Correlations', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Correlation Coefficient')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPositive Correlations (top 5):\")\n",
    "    for feat, corr in top_positive.items():\n",
    "        print(f\"  {feat:15s}: {corr:6.3f}\")\n",
    "    \n",
    "    print(\"\\nNegative Correlations (top 5):\")\n",
    "    for feat, corr in top_negative.items():\n",
    "        print(f\"  {feat:15s}: {corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-to-feature correlations (excluding target)\n",
    "feature_corr = correlation_matrix.drop(TARGET_COLUMN, axis=0).drop(TARGET_COLUMN, axis=1)\n",
    "\n",
    "# Get upper triangle (avoid duplicates)\n",
    "upper_tri = np.triu(np.ones_like(feature_corr), k=1).astype(bool)\n",
    "feature_pairs = feature_corr.where(upper_tri).stack().reset_index()\n",
    "feature_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "feature_pairs['Abs_Correlation'] = feature_pairs['Correlation'].abs()\n",
    "\n",
    "# Top correlated feature pairs\n",
    "top_pairs = feature_pairs.nlargest(10, 'Abs_Correlation')\n",
    "\n",
    "print(\"\\nüîó Top 10 Correlated Feature Pairs:\\n\")\n",
    "for idx, row in top_pairs.iterrows():\n",
    "    print(f\"  {row['Feature 1']:12s} ‚Üî {row['Feature 2']:12s}: {row['Correlation']:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bivariate Analysis: Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots: Numeric features by target class\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    df.boxplot(column=col, by=TARGET_COLUMN, ax=ax, patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "               medianprops=dict(color='red', linewidth=2))\n",
    "    \n",
    "    ax.set_title(f'{col} by {TARGET_COLUMN}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(f'{TARGET_COLUMN} (0=No Disease, 1=Disease)')\n",
    "    ax.set_ylabel(col)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks([1, 2], ['No Disease (0)', 'Disease (1)'])\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(numeric_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Numeric Features: Distribution by Target Class', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison by target class\n",
    "print(\"\\nüìä Mean Values by Target Class:\\n\")\n",
    "\n",
    "comparison_df = df.groupby(TARGET_COLUMN)[numeric_cols].mean().T\n",
    "comparison_df.columns = ['No Disease (0)', 'Disease (1)']\n",
    "comparison_df['Difference'] = comparison_df['Disease (1)'] - comparison_df['No Disease (0)']\n",
    "comparison_df = comparison_df.round(2)\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  - Positive difference: Higher values associated with disease\")\n",
    "print(\"  - Negative difference: Lower values associated with disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "print(\"\\nüîç Outlier Detection (IQR Method):\\n\")\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    n_outliers = len(outliers)\n",
    "    pct_outliers = (n_outliers / len(df) * 100)\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Lower Bound': round(lower_bound, 2),\n",
    "        'Upper Bound': round(upper_bound, 2),\n",
    "        'Outliers': n_outliers,\n",
    "        'Percentage': round(pct_outliers, 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "display(outlier_df)\n",
    "\n",
    "total_outliers = outlier_df['Outliers'].sum()\n",
    "print(f\"\\nTotal outlier data points: {total_outliers}\")\n",
    "if total_outliers < len(df) * 0.05:\n",
    "    print(\"‚úÖ Outliers are minimal (< 5% of data)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Consider outlier treatment or robust scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìå KEY INSIGHTS FROM EDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total samples: {len(df):,}\")\n",
    "print(f\"   - Features: {len(df.columns) - 1} ({len(numeric_cols)} numeric, {len(categorical_cols)} categorical)\")\n",
    "print(f\"   - Target: {TARGET_COLUMN} (binary classification)\")\n",
    "\n",
    "# Class balance\n",
    "balance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"\\n2. CLASS BALANCE:\")\n",
    "print(f\"   - No Disease (0): {class_counts[0]} ({class_pct[0]}%)\")\n",
    "print(f\"   - Disease (1): {class_counts[1]} ({class_pct[1]}%)\")\n",
    "print(f\"   - Balance ratio: {balance_ratio:.2f}\")\n",
    "if balance_ratio > 0.8:\n",
    "    print(f\"   ‚úÖ Well-balanced dataset, no special handling needed\")\n",
    "\n",
    "# Missing values\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"\\n3. DATA QUALITY:\")\n",
    "if total_missing == 0:\n",
    "    print(f\"   ‚úÖ No missing values detected\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Missing values: {total_missing} ({total_missing/df.size*100:.2f}%)\")\n",
    "\n",
    "# Top predictors\n",
    "if TARGET_COLUMN in correlation_matrix.columns:\n",
    "    top_3_positive = target_corr.head(3)\n",
    "    top_3_negative = target_corr.tail(3)\n",
    "    \n",
    "    print(f\"\\n4. TOP PREDICTIVE FEATURES:\")\n",
    "    print(f\"   Positive correlations:\")\n",
    "    for feat, corr in top_3_positive.items():\n",
    "        print(f\"     - {feat}: {corr:.3f}\")\n",
    "    print(f\"   Negative correlations:\")\n",
    "    for feat, corr in top_3_negative.items():\n",
    "        print(f\"     - {feat}: {corr:.3f}\")\n",
    "\n",
    "# Feature distributions\n",
    "print(f\"\\n5. FEATURE DISTRIBUTIONS:\")\n",
    "highly_skewed = stats_df[abs(stats_df['Skewness']) > 1].index.tolist()\n",
    "if highly_skewed:\n",
    "    print(f\"   ‚ö†Ô∏è  Highly skewed features: {', '.join(highly_skewed)}\")\n",
    "    print(f\"      ‚Üí Consider log transformation or robust scaling\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Most features are fairly symmetric\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n6. MODELING RECOMMENDATIONS:\")\n",
    "print(f\"   ‚úÖ Use stratified train/test split to maintain class balance\")\n",
    "print(f\"   ‚úÖ Apply StandardScaler for numeric features\")\n",
    "print(f\"   ‚úÖ Use OneHotEncoder for categorical features\")\n",
    "print(f\"   ‚úÖ Consider both linear (LogReg) and non-linear (RF) models\")\n",
    "print(f\"   ‚úÖ Use ROC-AUC as primary evaluation metric\")\n",
    "\n",
    "if highly_skewed:\n",
    "    print(f\"   ‚ö†Ô∏è  Consider PowerTransformer for skewed features\")\n",
    "\n",
    "if total_outliers > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Monitor outlier impact on model performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EDA COMPLETE\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save EDA Outputs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EDA artifacts directory\n",
    "from src.config import ARTIFACTS_DIR\n",
    "\n",
    "eda_dir = ARTIFACTS_DIR / 'eda'\n",
    "eda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüíæ Saving EDA outputs to: {eda_dir}\\n\")\n",
    "\n",
    "# Save summary statistics\n",
    "stats_df.to_csv(eda_dir / 'numeric_features_stats.csv')\n",
    "print(\"   ‚úÖ Saved: numeric_features_stats.csv\")\n",
    "\n",
    "# Save correlation matrix\n",
    "correlation_matrix.to_csv(eda_dir / 'correlation_matrix.csv')\n",
    "print(\"   ‚úÖ Saved: correlation_matrix.csv\")\n",
    "\n",
    "# Save outlier summary\n",
    "outlier_df.to_csv(eda_dir / 'outlier_summary.csv', index=False)\n",
    "print(\"   ‚úÖ Saved: outlier_summary.csv\")\n",
    "\n",
    "# Save class distribution\n",
    "class_dist = pd.DataFrame({\n",
    "    'Class': class_counts.index,\n",
    "    'Count': class_counts.values,\n",
    "    'Percentage': class_pct.values\n",
    "})\n",
    "class_dist.to_csv(eda_dir / 'class_distribution.csv', index=False)\n",
    "print(\"   ‚úÖ Saved: class_distribution.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All EDA outputs saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This EDA notebook analyzed the Heart Disease UCI dataset and revealed:\n",
    "\n",
    "- **Well-balanced** binary classification problem\n",
    "- **Clean data** with minimal missing values\n",
    "- **Strong predictive features** identified through correlation analysis\n",
    "- **Appropriate preprocessing** strategy defined (scaling + encoding)\n",
    "- **Ready for modeling** with confidence in data quality\n",
    "\n",
    "**Next Steps:**\n",
    "1. Feature engineering pipeline implementation\n",
    "2. Model training (Logistic Regression, Random Forest)\n",
    "3. Model evaluation and comparison\n",
    "4. MLflow experiment tracking\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
